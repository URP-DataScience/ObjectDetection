{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV-III_Object Detection_ulka.r.patil@gmail.com",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/URPDataScience/ObjectDetection/blob/main/CV_III_Object_Detection_ulka_r_patil_gmail_com.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6o7dw9Xynaj"
      },
      "source": [
        "Table of Contents\n",
        "1.\tIntroduction:\n",
        "\n",
        "2.Dataset\n",
        "\n",
        " 2.1 About Dataset\n",
        " \n",
        " 2.2 Objectives\n",
        "\n",
        "3.Object Detection using YOLOv3\n",
        "\n",
        "4.Download the Pre-trained Model Weights\n",
        "\n",
        "5.Creating and saving Model\n",
        "\n",
        "     o\t5.1 Utility Functions\n",
        "     \n",
        "     o\t5.2 Model Building\n",
        "     \n",
        "     o\t5.3 Saving the Model\n",
        "\n",
        "\n",
        "6.Load the Model\n",
        "\n",
        "7.Make a Prediction and Interpret Results\n",
        "\n",
        "     o\t7.1 Utility Functions\n",
        "    \n",
        "     o\t7.2 Helper Functions\n",
        "     \n",
        "     o\t7.3 Downloading the image for Object Detection\n",
        "     \n",
        "     o\t7.4 Performing object Detection\n",
        "\n",
        "8.Conclusion\n",
        "\n",
        "9.Object detection has found applications across industries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shx7p4XWArYU"
      },
      "source": [
        "1\tIntroduction: \n",
        "\n",
        "Object recognition is a general term to describe a collection of related computer vision tasks that involve identifying objects in digital photographs. Image classification involves predicting the class of one object in an image. Object localization refers to identifying the location of one or more objects in an image and drawing abounding box around their extent. Object detection combines these two tasks and localizes and classifies one or more objects in an image. When a user or practitioner refers to “object recognition, they often mean “object detection “.\n",
        "\n",
        "As such, we can distinguish between these three computer vision tasks:\n",
        "\n",
        "**•\tImage Classification:** Predict the type or class of an object in an image.\n",
        "\n",
        "      •\tInput: An image with a single object, such as a photograph.\n",
        "\n",
        "      •\tOutput: A class label (e.g. one or more integers that are mapped to   class labels).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "**•\tObject Localization:** Locate the presence of objects in an image and indicate their location with a bounding box.\n",
        "\n",
        "     •\tInput: An image with one or more objects, such as a photograph.\n",
        "\n",
        "     •\tOutput: One or more bounding boxes (e.g. defined by a point, width, and height).\n",
        "\n",
        "**•\tObject Detection:** Locate the presence of objects with a bounding box and types or classes of the located objects in an image.\n",
        "\n",
        "     •\tInput: An image with one or more objects, such as a photograph.\n",
        "\n",
        "     •\tOutput: One or more bounding boxes (e.g. defined by a point, width, and height), and a class label for each bounding box.\n",
        "\n",
        "•\tImage classification: \n",
        "\n",
        "Algorithms produce a list of object categories present in the image.\n",
        "\n",
        "•\tSingle-object localization: \n",
        "\n",
        "Algorithms produce a list of object categories present in the image, along with an axis-aligned bounding box indicating the position and scale of one instance of each object category.\n",
        "\n",
        "•\tObject detection: \n",
        "\n",
        "Algorithms produce a list of object categories present in the image along with an axis-aligned bounding box indicating the position and scale of every instance of each object category.\n",
        "\n",
        "People often confuse image classification and object detection scenarios. In general, if you want to classify an image into a certain category, you use image classification. On the other hand, if you aim to identify the location of objects in an image, and, for example, count the number of instances of an object, you can use object detection. There is, however, some overlap between these two scenarios. If you want to classify an image in to a certain category, it could happen that the object or the characteristics that are required to perform categorization are too small with respect to the full image. In that case, you would achieve better performance with object detection instead of image classification even if you are not interested in the exact location or counts of the object. Imagine you need to check circuit boards and classify them as either defect or correct. While it is essentially a classification problem, the defects might be too small to be noticeable with an image classification model. Constructing an object detection dataset will cost more time, yet it will result most likely in a better model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVu5zf9c2bNj"
      },
      "source": [
        "2.Dataset\n",
        "\n",
        "2.1 About Dataset:\n",
        "\n",
        "This dataset is an image set contains 32 unique images from the COCO image dataset.\n",
        "COCO stands for Common Objects in Context.\n",
        "COCO is a large-scale object detection, segmentation, and captioning dataset.\n",
        "The images present in the dataset contains various different objects like people, vehicles, animals, etc.\n",
        "\n",
        "\n",
        "2.2 Objectives:\n",
        "\n",
        "This dataset contains only a few images from the COCO dataset, and hence can’t be used to train an Object Detection model. An Object Detection model having pre-trained COCO dataset weights can be used to make predictions on this dataset. In this way, you can apply both Transfer Learning and Object Detection on this data. The final model should be able to detect different objects present in each image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBExfpY8C6r1"
      },
      "source": [
        "3. Object Detection using YOLOv3\n",
        "\n",
        "Popular family of object recognition models is referred to collectively as YOLO or “You Only Look Once,” developed by Joseph Redmon, et al.The R-CNN models may be generally more accurate, yet the YOLO family of models are fast, much faster than R-CNN, achieving object detection in real-time..\n",
        "The approach involves a single neural network trained end to end that takes a photograph as input and predicts bounding boxes and class labels for each bounding box directly. The technique offers lower predictive accuracy (e.g. more localization errors), although operates at 45 frames per second and up to 155 frames per second for a speed-optimized version of the model.\n",
        "\n",
        "•\tThe “You Only Look Once,” or YOLO, family of models are a series of end-to-end deep learning models designed for fast object detection.\n",
        "\n",
        "•\tThe approach involves a single deep convolutional neural network (originally a version of GoogLeNet, later updated and called DarkNet based on VGG) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification.\n",
        "\n",
        "•\tThe result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.\n",
        "\n",
        "•\tThere are three main variations of the approach, YOLOv1, YOLOv2, and YOLOv3.\n",
        "\n",
        "•\tWe will use experiencor’s keras-yolo3 project as the basis for performing object detection with a YOLOv3 model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvj-Xm7_ycNN"
      },
      "source": [
        "Algorithms based on regression – instead of selecting interesting parts of an image, they predict classes and bounding boxes for the whole image in one run of the algorithm. The two best known examples from this group are the YOLO (You Only Look Once) family algorithms and SSD (Single Shot Multibox Detector). They are commonly used for  real-time object detection as, in general, they trade a bit of accuracy for large improvements in speed.\n",
        "To understand the YOLO algorithm, it is necessary to establish what is actually being predicted. Ultimately, we aim to predict a class of an object and the bounding box specifying object location. Each bounding box can be described using four descriptors:\n",
        "1.\tcenter of a bounding box (bxby)\n",
        "2.\twidth (bw)\n",
        "3.\theight (bh)\n",
        "4.\tvalue cis corresponding to a class of an object (such as: car, traffic lights, etc.).\n",
        "In addition, we have to predict the pc value, which is the probability that there is an object in the bounding box.\n",
        "As we mentioned above, when working with the YOLO algorithm we are not searching for interesting regions in our image that could potentially contain an object. \n",
        "Instead, we are splitting our image into cells, typically using a 19×19 grid. Each cell is responsible for predicting 5 bounding boxes (in case there is more than one object in this cell). Therefore, we arrive at a large number of 1805 bounding boxes for one image.\n",
        "Most of these cells and bounding boxes will not contain an object. Therefore, we predict the value pc, which serves to remove boxes with low object probability and bounding boxes with the highest shared area in a process called non-max suppression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkP6ofJrTL7e"
      },
      "source": [
        "# 4. Download the Pre-trained Model Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAQq-Nk21EjM"
      },
      "source": [
        "- We will use `wget` to download the pre-trained model weights.\n",
        "\n",
        "- These were trained using the DarkNet code base on the **MSCOCO** dataset.\n",
        "\n",
        "- **Downloading** the weights might take upto **15 minutes**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py2hXDDM3n2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b83787e-3022-4fe8-c78f-9f68625c377c"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 14:12:26--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  75.0MB/s    in 3.4s    \n",
            "\n",
            "2021-08-08 14:12:29 (70.5 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XSW_UxXAESu"
      },
      "source": [
        "- After this the weights are saved in the current working directory under the name **yolov3.weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CEzGrtlgRh7"
      },
      "source": [
        "- We will also **download** a **python script** containing code which will help us **load** the above downloaded **weights** into our model.\n",
        "\n",
        "- To **simplify** the download, we have created a **zip** file containing the python script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VXVrwkjgRh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0655fc1-58ff-4222-b8d5-1c18d3388bfb"
      },
      "source": [
        "!wget https://github.com/insaid2018/DeepLearning/raw/master/Data/weight_reader.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 14:12:34--  https://github.com/insaid2018/DeepLearning/raw/master/Data/weight_reader.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/weight_reader.zip [following]\n",
            "--2021-08-08 14:12:34--  https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/weight_reader.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 879 [application/zip]\n",
            "Saving to: ‘weight_reader.zip’\n",
            "\n",
            "weight_reader.zip   100%[===================>]     879  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-08 14:12:35 (44.4 MB/s) - ‘weight_reader.zip’ saved [879/879]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MlpGDYzCp-e"
      },
      "source": [
        "!unzip -qq weight_reader.zip"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIkVbIUpDBfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b224490-6a8a-4818-8fe0-8bfca1ca26a5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2771749222_15312a0131_z.jpg  giraffe.jpg  sample_data\t    weight_reader.zip\n",
            "eagle.jpg\t\t     kite.jpg\t  weight_reader.py  yolov3.weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R16jx5eGC-F8"
      },
      "source": [
        "- After **unzipping** the `weight_reader.zip` file, we obtain the `weight_reader.py` file.\n",
        "\n",
        "- This file contains the **weight reading** code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq7xdWjy0vvi"
      },
      "source": [
        "# 5. Create and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYvYPQsx4uK4"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYh_W46_jFn"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from tensorflow.keras.layers import add, concatenate"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0q-FMZMkKNV"
      },
      "source": [
        "- We will import the **WeightReader** class from `weight_reader.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6YstlgbaUlf"
      },
      "source": [
        "from weight_reader import WeightReader"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18VpoGfM_6dG"
      },
      "source": [
        "- We will **create** a YOLOv3 Keras model and save it to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BRxt_bOKzI8"
      },
      "source": [
        "# 5.1 Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2-Han91Awlm"
      },
      "source": [
        "- We will create some functions `_conv_block` and `make_yolov3_model`, which will help us in building our **YOLOv3** model.\n",
        "\n",
        "- The YOLOv3 model is build using the **Keras Functional API**.\n",
        "\n",
        "- Simply, Keras Functional API is used to make **complex model architectures** which can't be build using the Sequential API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcW6q_bpBD3H"
      },
      "source": [
        "# Function to help create custom convolution blocks\n",
        "def _conv_block(inp, convs, skip=True):\n",
        "\tx = inp\n",
        "\tcount = 0\n",
        "\tfor conv in convs:\n",
        "\t\tif count == (len(convs) - 2) and skip:\n",
        "\t\t\tskip_connection = x\n",
        "\t\tcount += 1\n",
        "\t\tif conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
        "\t\tx = Conv2D(conv['filter'],\n",
        "\t\t\t\t   conv['kernel'],\n",
        "\t\t\t\t   strides=conv['stride'],\n",
        "\t\t\t\t   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
        "\t\t\t\t   name='conv_' + str(conv['layer_idx']),\n",
        "\t\t\t\t   use_bias=False if conv['bnorm'] else True)(x)\n",
        "\t\tif conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
        "\t\tif conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
        "\treturn add([skip_connection, x]) if skip else x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6VFAeYYBIwN"
      },
      "source": [
        "# Function to make the yolov3 model architecture\n",
        "def make_yolov3_model():\n",
        "\tinput_image = Input(shape=(None, None, 3))\n",
        "\t# Layer  0 => 4\n",
        "\tx = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
        "\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
        "\t# Layer  5 => 8\n",
        "\tx = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
        "\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
        "\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
        "\t# Layer  9 => 11\n",
        "\tx = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
        "\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
        "\t# Layer 12 => 15\n",
        "\tx = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
        "\t\t\t\t\t\t{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
        "\t# Layer 16 => 36\n",
        "\tfor i in range(7):\n",
        "\t\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
        "\tskip_36 = x\n",
        "\t# Layer 37 => 40\n",
        "\tx = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
        "\t# Layer 41 => 61\n",
        "\tfor i in range(7):\n",
        "\t\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
        "\tskip_61 = x\n",
        "\t# Layer 62 => 65\n",
        "\tx = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
        "\t# Layer 66 => 74\n",
        "\tfor i in range(3):\n",
        "\t\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
        "\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
        "\t# Layer 75 => 79\n",
        "\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
        "\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
        "\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
        "\t# Layer 80 => 82\n",
        "\tyolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
        "\t\t\t\t\t\t\t  {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
        "\t# Layer 83 => 86\n",
        "\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
        "\tx = UpSampling2D(2)(x)\n",
        "\tx = concatenate([x, skip_61])\n",
        "\t# Layer 87 => 91\n",
        "\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
        "\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
        "\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
        "\t# Layer 92 => 94\n",
        "\tyolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
        "\t\t\t\t\t\t\t  {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
        "\t# Layer 95 => 98\n",
        "\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
        "\tx = UpSampling2D(2)(x)\n",
        "\tx = concatenate([x, skip_36])\n",
        "\t# Layer 99 => 106\n",
        "\tyolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
        "\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
        "\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
        "\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
        "\t\t\t\t\t\t\t   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
        "\tmodel = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
        "\treturn model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMOda2lK5MG"
      },
      "source": [
        "\n",
        "# 5.2 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KF1oWlK8Io"
      },
      "source": [
        "- We will build the model using the `make_yolov3_model` function.\n",
        "\n",
        "- Then we will load the pre-trained weights into the model using the **WeightReader**'s `load_weights` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsHz8XJDCAgV"
      },
      "source": [
        "# Define the model\n",
        "model = make_yolov3_model()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1U1SSNECAjE"
      },
      "source": [
        "# Load the model weights\n",
        "weight_reader = WeightReader('yolov3.weights')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aSHCQ6VCAvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a5b5bd-7efb-4016-ff15-68bd15419acd"
      },
      "source": [
        "# Set the model weights into the model\n",
        "weight_reader.load_weights(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading weights of convolution #0\n",
            "loading weights of convolution #1\n",
            "loading weights of convolution #2\n",
            "loading weights of convolution #3\n",
            "no convolution #4\n",
            "loading weights of convolution #5\n",
            "loading weights of convolution #6\n",
            "loading weights of convolution #7\n",
            "no convolution #8\n",
            "loading weights of convolution #9\n",
            "loading weights of convolution #10\n",
            "no convolution #11\n",
            "loading weights of convolution #12\n",
            "loading weights of convolution #13\n",
            "loading weights of convolution #14\n",
            "no convolution #15\n",
            "loading weights of convolution #16\n",
            "loading weights of convolution #17\n",
            "no convolution #18\n",
            "loading weights of convolution #19\n",
            "loading weights of convolution #20\n",
            "no convolution #21\n",
            "loading weights of convolution #22\n",
            "loading weights of convolution #23\n",
            "no convolution #24\n",
            "loading weights of convolution #25\n",
            "loading weights of convolution #26\n",
            "no convolution #27\n",
            "loading weights of convolution #28\n",
            "loading weights of convolution #29\n",
            "no convolution #30\n",
            "loading weights of convolution #31\n",
            "loading weights of convolution #32\n",
            "no convolution #33\n",
            "loading weights of convolution #34\n",
            "loading weights of convolution #35\n",
            "no convolution #36\n",
            "loading weights of convolution #37\n",
            "loading weights of convolution #38\n",
            "loading weights of convolution #39\n",
            "no convolution #40\n",
            "loading weights of convolution #41\n",
            "loading weights of convolution #42\n",
            "no convolution #43\n",
            "loading weights of convolution #44\n",
            "loading weights of convolution #45\n",
            "no convolution #46\n",
            "loading weights of convolution #47\n",
            "loading weights of convolution #48\n",
            "no convolution #49\n",
            "loading weights of convolution #50\n",
            "loading weights of convolution #51\n",
            "no convolution #52\n",
            "loading weights of convolution #53\n",
            "loading weights of convolution #54\n",
            "no convolution #55\n",
            "loading weights of convolution #56\n",
            "loading weights of convolution #57\n",
            "no convolution #58\n",
            "loading weights of convolution #59\n",
            "loading weights of convolution #60\n",
            "no convolution #61\n",
            "loading weights of convolution #62\n",
            "loading weights of convolution #63\n",
            "loading weights of convolution #64\n",
            "no convolution #65\n",
            "loading weights of convolution #66\n",
            "loading weights of convolution #67\n",
            "no convolution #68\n",
            "loading weights of convolution #69\n",
            "loading weights of convolution #70\n",
            "no convolution #71\n",
            "loading weights of convolution #72\n",
            "loading weights of convolution #73\n",
            "no convolution #74\n",
            "loading weights of convolution #75\n",
            "loading weights of convolution #76\n",
            "loading weights of convolution #77\n",
            "loading weights of convolution #78\n",
            "loading weights of convolution #79\n",
            "loading weights of convolution #80\n",
            "loading weights of convolution #81\n",
            "no convolution #82\n",
            "no convolution #83\n",
            "loading weights of convolution #84\n",
            "no convolution #85\n",
            "no convolution #86\n",
            "loading weights of convolution #87\n",
            "loading weights of convolution #88\n",
            "loading weights of convolution #89\n",
            "loading weights of convolution #90\n",
            "loading weights of convolution #91\n",
            "loading weights of convolution #92\n",
            "loading weights of convolution #93\n",
            "no convolution #94\n",
            "no convolution #95\n",
            "loading weights of convolution #96\n",
            "no convolution #97\n",
            "no convolution #98\n",
            "loading weights of convolution #99\n",
            "loading weights of convolution #100\n",
            "loading weights of convolution #101\n",
            "loading weights of convolution #102\n",
            "loading weights of convolution #103\n",
            "loading weights of convolution #104\n",
            "loading weights of convolution #105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJDzhGeMLS2C"
      },
      "source": [
        "# 5.3 Saving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgN3tmZvLgc6"
      },
      "source": [
        "- After loading the weights, we will save our model into a file for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4nDyzNP4R6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34c2a07-ca55-4a0f-85f8-e435d584036a"
      },
      "source": [
        "# Save the model to file\n",
        "model.save('model.h5')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJcUlaWSD2dA"
      },
      "source": [
        "- The model is saved as model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJU4JK420xK6"
      },
      "source": [
        "\n",
        "# 6. Load the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdAUrLVDV6Q"
      },
      "source": [
        "- Now we will load the YOLOv3 model and perform Object Detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpCXEofPEQ1w"
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-SGY8h-Dvzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c4b4fb-481f-45c3-8c3f-106a3cfbf47c"
      },
      "source": [
        "# Loading the yolov3 model\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY_CW7aRExNJ"
      },
      "source": [
        "- Our yolov3 model is loaded into the `model` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzWkMzYV0xc-"
      },
      "source": [
        "\n",
        "# 7. Make a Prediction and Interpret Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8znTVilEKcX"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vmwNZ78EKex"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p02azhYJZJ7"
      },
      "source": [
        "\n",
        "# 7.1 Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu9MN-wbFDdu"
      },
      "source": [
        "- We will create some **utility** functions for our YOLOv3 algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrJW4PsEKip"
      },
      "source": [
        "# Creating BoundBox class for creating the bounding boxes around the objects\n",
        "class BoundBox:\n",
        "\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "\t\tself.xmin = xmin\n",
        "\t\tself.ymin = ymin\n",
        "\t\tself.xmax = xmax\n",
        "\t\tself.ymax = ymax\n",
        "\t\tself.objness = objness\n",
        "\t\tself.classes = classes\n",
        "\t\tself.label = -1\n",
        "\t\tself.score = -1\n",
        "\n",
        "\tdef get_label(self):\n",
        "\t\tif self.label == -1:\n",
        "\t\t\tself.label = np.argmax(self.classes)\n",
        "\n",
        "\t\treturn self.label\n",
        "\n",
        "\tdef get_score(self):\n",
        "\t\tif self.score == -1:\n",
        "\t\t\tself.score = self.classes[self.get_label()]\n",
        "\n",
        "\t\treturn self.score"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdv5NlEqEbSe"
      },
      "source": [
        "def _sigmoid(x):\n",
        "\treturn 1. / (1. + np.exp(-x))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0gHlTtEbVX"
      },
      "source": [
        "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
        "\tgrid_h, grid_w = netout.shape[:2]\n",
        "\tnb_box = 3\n",
        "\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "\tnb_class = netout.shape[-1] - 5\n",
        "\tboxes = []\n",
        "\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "\tnetout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        "\n",
        "\tfor i in range(grid_h*grid_w):\n",
        "\t\trow = i / grid_w\n",
        "\t\tcol = i % grid_w\n",
        "\t\tfor b in range(nb_box):\n",
        "\t\t\t# 4th element is objectness score\n",
        "\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n",
        "\t\t\tif(objectness.all() <= obj_thresh): continue\n",
        "\t\t\t# First 4 elements are x, y, w, and h\n",
        "\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n",
        "\t\t\tx = (col + x) / grid_w # center position, unit: image width\n",
        "\t\t\ty = (row + y) / grid_h # center position, unit: image height\n",
        "\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
        "\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
        "\t\t\t# Last elements are class probabilities\n",
        "\t\t\tclasses = netout[int(row)][col][b][5:]\n",
        "\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "\t\t\tboxes.append(box)\n",
        "\treturn boxes"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMUcgD9eEeOo"
      },
      "source": [
        "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
        "\tnew_w, new_h = net_w, net_h\n",
        "\tfor i in range(len(boxes)):\n",
        "\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXlmsEckEeRG"
      },
      "source": [
        "def _interval_overlap(interval_a, interval_b):\n",
        "\tx1, x2 = interval_a\n",
        "\tx3, x4 = interval_b\n",
        "\tif x3 < x1:\n",
        "\t\tif x4 < x1:\n",
        "\t\t\treturn 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x1\n",
        "\telse:\n",
        "\t\tif x2 < x3:\n",
        "\t\t\t return 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x3"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REr1aYIMEeUO"
      },
      "source": [
        "def bbox_iou(box1, box2):\n",
        "\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\tintersect = intersect_w * intersect_h\n",
        "\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\tunion = w1*h1 + w2*h2 - intersect\n",
        "\treturn float(intersect) / union"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKucv1MEbYf"
      },
      "source": [
        "def do_nms(boxes, nms_thresh):\n",
        "\tif len(boxes) > 0:\n",
        "\t\tnb_class = len(boxes[0].classes)\n",
        "\telse:\n",
        "\t\treturn\n",
        "\tfor c in range(nb_class):\n",
        "\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "\t\tfor i in range(len(sorted_indices)):\n",
        "\t\t\tindex_i = sorted_indices[i]\n",
        "\t\t\tif boxes[index_i].classes[c] == 0: continue\n",
        "\t\t\tfor j in range(i+1, len(sorted_indices)):\n",
        "\t\t\t\tindex_j = sorted_indices[j]\n",
        "\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "\t\t\t\t\tboxes[index_j].classes[c] = 0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ueAarMJJljA"
      },
      "source": [
        "\n",
        "# 7.2 Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or0bUOFcFiHg"
      },
      "source": [
        "- Now we will create some helper functions to **load** the image and **visualize** the object detection results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YjpU_KUEmLy"
      },
      "source": [
        "# Load and prepare an image\n",
        "def load_image_pixels(filename, shape):\n",
        "\t# Load the image to get its shape\n",
        "\timage = load_img(filename)\n",
        "\twidth, height = image.size\n",
        "\t# Load the image with the required size\n",
        "\timage = load_img(filename, target_size=shape)\n",
        "\t# Convert to numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# Scale pixel values to [0, 1]\n",
        "\timage = image.astype('float32')\n",
        "\timage /= 255.0\n",
        "\t# Add a dimension so that we have one sample\n",
        "\timage = expand_dims(image, 0)\n",
        "\treturn image, width, height"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9_Ppeq_EmOH"
      },
      "source": [
        "# Get all of the results above a threshold\n",
        "def get_boxes(boxes, labels, thresh):\n",
        "\tv_boxes, v_labels, v_scores = list(), list(), list()\n",
        "\t# Enumerate all boxes\n",
        "\tfor box in boxes:\n",
        "\t\t# Enumerate all possible labels\n",
        "\t\tfor i in range(len(labels)):\n",
        "\t\t\t# Check if the threshold for this label is high enough\n",
        "\t\t\tif box.classes[i] > thresh:\n",
        "\t\t\t\tv_boxes.append(box)\n",
        "\t\t\t\tv_labels.append(labels[i])\n",
        "\t\t\t\tv_scores.append(box.classes[i]*100)\n",
        "\t\t\t\t# Don't break, many labels may trigger for one box\n",
        "\treturn v_boxes, v_labels, v_scores"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9pDhxQ_EmRq"
      },
      "source": [
        "# Draw all results\n",
        "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
        "\t# Load the image\n",
        "\tdata = plt.imread(filename)\n",
        "\t# Plot the image\n",
        "\tplt.imshow(data)\n",
        "\t# Get the context for drawing boxes\n",
        "\tax = plt.gca()\n",
        "\t# Plot each box\n",
        "\tfor i in range(len(v_boxes)):\n",
        "\t\tbox = v_boxes[i]\n",
        "\t\t# Get coordinates\n",
        "\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\t\t# Calculate width and height of the box\n",
        "\t\twidth, height = x2 - x1, y2 - y1\n",
        "\t\t# Create the shape\n",
        "\t\t# Can change the bounding box color here (currently red).\n",
        "\t\trect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "\t\t# Draw the box\n",
        "\t\tax.add_patch(rect)\n",
        "\t\t# Draw text and score in top left corner\n",
        "\t\t# Can change the label text color here (currently white).\n",
        "\t\tlabel = \"%s (%.1f)\" % (v_labels[i], v_scores[i])\n",
        "\t\tplt.text(x1, y1, label, color='white')\n",
        "\t# Show the plot\n",
        "\tplt.show()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCoVj4UAJqhR"
      },
      "source": [
        "\n",
        "# 7.3 Downloading the Image for Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x38xjGC-eTj"
      },
      "source": [
        "!wget -q https://github.com/URPDataScience/Object-Detection/blob/main/2771749222_15312a0131_z.jpg"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdlwYE8OExcQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtK5e9Us1KaB"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/kite.jpg\n",
        "!wget -q https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/eagle.jpg\n",
        "!wget -q https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/giraffe.jpg"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jheFfUkfJy36"
      },
      "source": [
        "\n",
        "# 7.4 Performing Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unczy_-1RcEU"
      },
      "source": [
        "# Define the expected input shape for the model\n",
        "input_w, input_h = 416, 416"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf6gFKmmRcJ6"
      },
      "source": [
        "# Define our new photo\n",
        "photo_filename = 'https://github.com/URPDataScience/Object-Detection/blob/main/2771749222_15312a0131_z.jpg'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfBmC2ybRcHJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "451ad972-3b9d-4a1c-906b-2602d63e4ea3"
      },
      "source": [
        "# Load and prepare image\n",
        "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-462fb8877b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and prepare image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8ebca9d51183>\u001b[0m in \u001b[0;36mload_image_pixels\u001b[0;34m(filename, shape)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Load the image to get its shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Load the image with the required size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    294\u001b[0m   \"\"\"\n\u001b[1;32m    295\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 296\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://github.com/URPDataScience/Object-Detection/blob/main/2771749222_15312a0131_z.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc7tnpa9Rfzl"
      },
      "source": [
        "# Make prediction\n",
        "yhat = model.predict(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR54nZ-tRf2b"
      },
      "source": [
        "# Summarize the shape of the list of arrays\n",
        "print([a.shape for a in yhat])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W730hkHjRf52"
      },
      "source": [
        "# Define the anchors\n",
        "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSLzsewQRf9Q"
      },
      "source": [
        "# Define the probability threshold for detected objects\n",
        "class_threshold = 0.6\n",
        "boxes = list()\n",
        "for i in range(len(yhat)):\n",
        "\t# decode the output of the network\n",
        "\tboxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zVnwhvRSOKD"
      },
      "source": [
        "# Correct the sizes of the bounding boxes for the shape of the image\n",
        "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lSrMzh6SOM4"
      },
      "source": [
        "# Suppress non-maximal boxes\n",
        "do_nms(boxes, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pSzD1d13nzv"
      },
      "source": [
        "# Define the labels for the classes we want to predict\n",
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \n",
        "          \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \n",
        "          \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \n",
        "          \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \n",
        "          \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \n",
        "          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \n",
        "          \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \n",
        "          \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \n",
        "          \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \n",
        "          \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SzRPc0RRs5Q"
      },
      "source": [
        "# Get the details of the detected objects\n",
        "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLrIum3eRpQK"
      },
      "source": [
        "# Summarize what we found\n",
        "for i in range(len(v_boxes)):\n",
        "\tprint(v_labels[i], v_scores[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csztfaYLRpSv"
      },
      "source": [
        "# Draw what we found\n",
        "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX7bBdwPTrbu"
      },
      "source": [
        "- Our YOLOv3 model is working fine, and is making accurate predictions.\n",
        "\n",
        "- We can see **bounding box** around each object with the *class name* and the *class probability* at the **top left** corner of the box.\n",
        "\n",
        "- Similar predictions can be made on other images as well.\n",
        "\n",
        "\n",
        "  - Change the **photo_filename** to any of these 3 images and **re-run** the code from that cell, and you'll be able to make predictions on them as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIjat0z5GE9Y"
      },
      "source": [
        "# 8. Conclusion :\n",
        "\n",
        "All the pictures are identified properly by the model.\n",
        "Object detection is a technique of the AI subset computer vision that is concerned with identifying objects and defining those by placing into distinct categories such as humans, cars, animals etc.\n",
        "\n",
        "It combines machine learning and deep learning to enable machines to identify different objects.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQdVR5s-tDiE"
      },
      "source": [
        "# 9. Object detection has found applications across industries.\n",
        "\n",
        "\n",
        "**Tracking Objects:**\n",
        "With object tracking it would be easier to track a person in a video. Object tracking could also be used in tracking the motion of a ball during a match. In the field of traffic monitoring too object tracking plays a crucial role.\n",
        "\n",
        "**Counting the crowd:**\n",
        "Crowd counting or people counting is another significant application of object detection. During a big festival, or, in a crowded mall this application comes in handy as it helps in dissecting the crowd and measure different groups.\n",
        "\n",
        "**Self-driving cars:**\n",
        "Another unique application of object detection technique is definitely self-driving cars.  A self-driving car can only navigate through a street safely if it could detect all the objects such as people, other cars, road signs on the road, in order to decide what action to take.\n",
        "\n",
        "**Detecting a vehicle:**\n",
        "In a road full of speeding vehicles object detection can help in a big way by tracking a particular vehicle and even its number plate. So, if a car gets into an accident or, breaks traffic rules then it is easier to detect that particular car using object detection model and thereby decreasing the rate of crime while enhancing security.\n",
        "\n",
        "**Detecting anomaly**\n",
        "Another useful application of object detection is definitely spotting an anomaly and it has industry specific usages.  For instance, in the field of agriculture object detection helps in identifying infected crops and thereby helps the farmers take measures accordingly.  It could also help identify skin problems in healthcare.  In the manufacturing industry the object detection technique can help in detecting problematic parts really fast and thereby allow the company to take the right step.\n",
        "Object detection technology has the potential to transform our world in multiple ways. However, the models still need to be developed further so that these can be applied across devices and platforms in real-time to offer cutting-edge solutions. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}